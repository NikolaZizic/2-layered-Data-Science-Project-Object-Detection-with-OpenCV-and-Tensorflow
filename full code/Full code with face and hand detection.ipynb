{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b36090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_MODEL_NAME = 'my_ssd_mobnet' \n",
    "PRETRAINED_MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8' #name of our pre-trained model\n",
    "PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz'\n",
    "TF_RECORD_SCRIPT_NAME = 'generate_tfrecord.py'\n",
    "LABEL_MAP_NAME = 'label_map.pbtxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f087e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    'WORKSPACE_PATH': os.path.join('Tensorflow', 'workspace'),\n",
    "    'SCRIPTS_PATH': os.path.join('Tensorflow','scripts'),\n",
    "    'APIMODEL_PATH': os.path.join('Tensorflow','models'),\n",
    "    'ANNOTATION_PATH': os.path.join('Tensorflow', 'workspace','annotations'),\n",
    "    'IMAGE_PATH': os.path.join('Tensorflow', 'workspace','images'),\n",
    "    'MODEL_PATH': os.path.join('Tensorflow', 'workspace','models'),\n",
    "    'PRETRAINED_MODEL_PATH': os.path.join('Tensorflow', 'workspace','pre-trained-models'),\n",
    "    'CHECKPOINT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME), \n",
    "    'OUTPUT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'export'), \n",
    "    'TFJS_PATH':os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'tfjsexport'), \n",
    "    'TFLITE_PATH':os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'tfliteexport'), \n",
    "    'PROTOC_PATH':os.path.join('Tensorflow','protoc')\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9968a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {\n",
    "    'PIPELINE_CONFIG':os.path.join('Tensorflow', 'workspace','models', CUSTOM_MODEL_NAME, 'pipeline.config'),\n",
    "    'TF_RECORD_SCRIPT': os.path.join(paths['SCRIPTS_PATH'], TF_RECORD_SCRIPT_NAME), \n",
    "    'LABELMAP': os.path.join(paths['ANNOTATION_PATH'], LABEL_MAP_NAME)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40263c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2679c54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(paths['CHECKPOINT_PATH'], 'ckpt-3')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0db5055",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(files['LABELMAP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a5892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import time\n",
    "%matplotlib inline\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "import math\n",
    "import statistics as stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b0cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "caspath = 'C:/Users/850/anaconda3/envs/tfod/Lib/site-packages/cv2/data/haarcascade_frontalface_alt2.xml'\n",
    "face_cascade = cv2.CascadeClassifier(caspath)\n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "recognizer.read(\"trainner.yml\")\n",
    "labels = {}\n",
    "with open(\"labels.pickle\", 'rb') as f:\n",
    "    og_lables = pickle.load(f)\n",
    "    labels = {v:k for k,v in og_lables.items()} # to invert lables \n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "stor_conf = []\n",
    "while True:\n",
    "    key = cv2.waitKey(20) & 0xFF\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor = 1.5,minNeighbors = 5 )\n",
    "    for (x, y, w, h) in faces:\n",
    "        #print (x, y , w, h)\n",
    "        roi_gray = gray[y:y+h, x:x+w] # roi is region of interst\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        canny = cv2.Canny(frame,100,100)\n",
    "        id_, conf = recognizer.predict(roi_gray)\n",
    "        \n",
    "        \n",
    "     \n",
    "            \n",
    "        if conf >= 60 and id_ == 0:\n",
    "            print(id_)           \n",
    "            print(labels[id_], conf)\n",
    "            stor_conf.append(conf)\n",
    "            \n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            name = labels[id_]\n",
    "            color = (255,255,255)\n",
    "            stroke = 2 \n",
    "            \n",
    "            cv2.putText(frame, name, (x,y), font, 1, color, stroke, cv2.LINE_AA)\n",
    "        if id_ != 0 or conf < 60:\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            color = (255,255,255)\n",
    "            stroke = 2 \n",
    "            cv2.putText(frame, \"Unknown\", (x,y), font, 1, color, stroke, cv2.LINE_AA)\n",
    "        \n",
    "            \n",
    "            \n",
    "        color = (255,0,0)\n",
    "        stroke = 2\n",
    "        end_cord_x = x+w #width\n",
    "        end_cord_y = y+h #height\n",
    "        cv2.rectangle(frame, (x, y), (end_cord_x, end_cord_y), color, stroke)\n",
    "    \n",
    "    cv2.imshow('frame', frame)\n",
    "   \n",
    "    if len(stor_conf) > 75 and stat.mean(stor_conf) > 70:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        devices = AudioUtilities.GetSpeakers()\n",
    "        interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "        volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "\n",
    "\n",
    "        cap1 = cv2.VideoCapture(0)\n",
    "        width = int(cap1.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap1.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "        n = 0 # We will need p and n this for the audio control\n",
    "        p = 0\n",
    "\n",
    "        while cap1.isOpened(): \n",
    "            ret, frame = cap1.read()\n",
    "            image_np = np.array(frame)\n",
    "\n",
    "            input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "            detections = detect_fn(input_tensor)\n",
    "\n",
    "            num_detections = int(detections.pop('num_detections'))\n",
    "            detections = {key: value[0, :num_detections].numpy()\n",
    "                          for key, value in detections.items()}\n",
    "            detections['num_detections'] = num_detections\n",
    "\n",
    "            # detection_classes should be ints.\n",
    "            detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "            label_id_offset = 1 # we have labeled our hand detection from 1 so we cn't have it start from 0.\n",
    "            image_np_with_detections = image_np.copy()\n",
    "            \n",
    "            #defining boxes and labels when activating our webcam.\n",
    "            viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                        image_np_with_detections,\n",
    "                        detections['detection_boxes'],\n",
    "                        detections['detection_classes']+label_id_offset,\n",
    "                        detections['detection_scores'],\n",
    "                        category_index,\n",
    "                        use_normalized_coordinates=True,\n",
    "                        max_boxes_to_draw=5,\n",
    "                        min_score_thresh=.8,\n",
    "                        agnostic_mode=False)\n",
    "\n",
    "            theLabels = detections['detection_classes']+label_id_offset\n",
    "            labelsList = theLabels.tolist()\n",
    "\n",
    "            #   This next part is for the audio change. We assign one hand label for volume up and another one\n",
    "            # for volume down.\n",
    "            if labelsList[0] == 1:\n",
    "                n += 5\n",
    "            if labelsList[0] == 2:\n",
    "                p += 5\n",
    "            print(detections['detection_classes']+label_id_offset)\n",
    "            if n >= 30:\n",
    "                currentVolumeDb = volume.GetMasterVolumeLevel()\n",
    "                volume.SetMasterVolumeLevel(currentVolumeDb + 0.5, None) # 0.5 represents the volume change when recognising the label\n",
    "                print(currentVolumeDb) \n",
    "                n = 0\n",
    "\n",
    "            if p >= 30:\n",
    "                currentVolumeDb = volume.GetMasterVolumeLevel()\n",
    "                volume.SetMasterVolumeLevel(currentVolumeDb - 0.5, None)\n",
    "                print(currentVolumeDb)\n",
    "                p = 0\n",
    "\n",
    "            cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 600)))\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                cap1.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "            if  key == ord('q'):\n",
    "                break\n",
    "    \n",
    "     \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ed5919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfod",
   "language": "python",
   "name": "tfod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
